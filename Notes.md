## 1. Extractive and Generative Question-Answering

Extractive and generative question-answering are two approaches used in natural language processing (NLP) to handle the task of providing answers to questions posed by humans. They differ primarily in how they find and present answers:

### Extractive Question-Answering

In extractive question-answering, the system looks for answers within a given text or set of documents and extracts the exact snippet (or snippets) that contain the answer. This approach assumes that the answer to the question is explicitly stated in the text. The system identifies the portion of the text that most likely answers the question and returns it verbatim. This method is effective for questions where specific information is requested, such as facts, dates, names, and figures.

**Key Points:**

* Answers are directly extracted from the provided text.
* The system does not generate new text; it only finds and returns the part of the text that contains the answer.
* It's suitable for "what," "who," "when," and "where" questions that have direct answers within the text.


### Generative Question-Answering

Generative question-answering goes a step further by producing answers that may not be explicitly stated in the source material. This approach uses understanding and synthesis capabilities to generate new responses that are coherent and relevant to the question, even if the exact wording isn't found in the text. This method is useful for more complex questions that require interpretation, inference, or summarization to answer. Generative models can combine information from different parts of the text or even external knowledge not present in the source material to create their answers.

**Key Points:**

* Answers are generated by the system, potentially synthesizing information from various parts of the text or even external sources.
* The system can produce answers that are not found verbatim in the source material, allowing for more flexible and comprehensive responses.
* It's suitable for "how," "why," and other explanatory or complex questions that require synthesis of information and reasoning.

### Summary
Extractive Question-Answering: Directly pulls answers from the source text, suitable for straightforward questions with explicit answers.
Generative Question-Answering: Generates new answers, often synthesizing information from various inputs, suitable for complex questions requiring inference or explanation.
The choice between extractive and generative question-answering models depends on the nature of the questions being asked and the complexity of the answers required. Generative models, powered by advanced AI techniques like those found in GPT (Generative Pre-trained Transformer) architectures, are becoming increasingly capable, providing more nuanced and contextually relevant answers.

### Sources:
* https://towardsdatascience.com/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a
* https://huggingface.co/tasks/question-answering

## 2. Pretrained Models Infromation

| Architecture |  Model name           | Details of the model |
|--------------|-----------------------|----------------------|
| BERT         | bert-base-uncased     | 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text |
| Bart         | facebook/bart-base    | 12-layer, 768-hidden, 16-heads, 139M parameters    |
| GPT-2        | gpt2                  | 12-layer, 768-hidden, 12-heads, 117M parameters. OpenAI GPT-2 English model |

## 3. Parallel GPU Training

When training large models, fitting larger batch sizes, or trying to increase throughput using multi-GPU compute, Lightning provides advanced optimized distributed training strategies to support these cases and offer substantial improvements in memory usage.

More details can be find here: https://lightning.ai/docs/pytorch/1.6.5/advanced/model_parallel.html#choosing-an-advanced-distributed-gpu-strategy

### Sources:
https://huggingface.co/transformers/v3.4.0/pretrained_models.html